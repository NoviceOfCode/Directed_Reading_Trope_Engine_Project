{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5S7Cihy01ew"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZfm6KmE0uts",
    "outputId": "9657b1d4-368c-4d0c-f66f-4c0d510d4bf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\jacob\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.40.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_rSzU9905nI",
    "outputId": "8ef2bd21-00bd-4972-8c80-894e260acba9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\anaconda3\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from typing import List\n",
    "import string\n",
    "import spacy\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "45BjAMzK16mh"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data_raw/parsed_tropes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJvd_45YuXe3",
    "outputId": "6fbefa80-ee50-48e2-8b83-91a0629eb156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29903 entries, 0 to 29902\n",
      "Data columns (total 8 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   trope_name                  29903 non-null  object\n",
      " 1   trope_link                  29903 non-null  object\n",
      " 2   trope_description           29903 non-null  object\n",
      " 3   related_link_names          29903 non-null  object\n",
      " 4   related_links               29903 non-null  object\n",
      " 5   example_descriptions        29903 non-null  object\n",
      " 6   example_related_links       29903 non-null  object\n",
      " 7   example_related_link_names  29903 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QllgwiPVR88y"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pydircZDR8vq"
   },
   "outputs": [],
   "source": [
    "def remove_unneeded_phrases(trope_link, description):\n",
    "  #Link of page was commonly in the text\n",
    "  modified_description = description.replace(trope_link, '')\n",
    "  #Articles often started witht he below sentences in the beginning\n",
    "  standard_beginning_text = \", 'Go To', '&#010;&#010\"\n",
    "  modified_description = re.sub(r'Go To', '', modified_description, count=1)\n",
    "  modified_description = modified_description.replace(standard_beginning_text, '')\n",
    "  modified_description = re.sub(r'This page has been alphabetized. Please add new examples in the correct order.','',\n",
    "                                modified_description, count=1)\n",
    "  modified_description = re.sub(r'Thanks!','',\n",
    "                                modified_description, count=1)\n",
    "  modified_description = re.sub(r\"Image removed per Image Pickin' thread\",'',\n",
    "                                modified_description, count=1)\n",
    "  modified_description = re.sub(r\"Image selected per Image Pickin' thread\",'',\n",
    "                                modified_description, count=1)\n",
    "  modified_description = re.sub(r\"Image replaced per Image Pickin' thread\",'',\n",
    "                                modified_description, count=1)\n",
    "  modified_description = re.sub(r\"Image replaced per Image Pickin' thread\",'',\n",
    "                                modified_description, count=1)\n",
    "  pattern = r'8212;index&#8212;&#8212;/index&#8212;'\n",
    "  modified_description = re.sub(pattern,'',\n",
    "                                modified_description, count=1)\n",
    "  url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|www\\.[a-zA-Z0-9-]+(\\.[a-zA-Z]{2,})+'\n",
    "  modified_description = re.sub(url_pattern,'',\n",
    "                                modified_description)\n",
    "  # HTML tags accidently scraped\n",
    "  tag_pattern = r'<[^>]+>.*?</[^>]+>|<[^>]+/?>'\n",
    "  modified_description = re.sub(tag_pattern,'',\n",
    "                                modified_description)\n",
    "  modified_description = re.sub(r\"Image selected via crowner in the Image Suggestion thread\",'',\n",
    "                                modified_description, count=1)\n",
    "  modified_description = re.sub(r\"do not change or remove without starting a new thread\",'',\n",
    "                                modified_description, count=1)\n",
    "  modified_description = re.sub(r\"This is based on opinion. Please don't list it on a work's trope example list\",'',\n",
    "                                modified_description, count=1)\n",
    "  modified_description = re.sub(r\"This list of examples has been alphabetized. Please add your example in the proper place\",'',\n",
    "                                modified_description, count=1)\n",
    "  modified_description = re.sub(r'&#\\d+;', '', modified_description)\n",
    "  return modified_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Gc5it9ArgUGY"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IPVPdJIaf9jp"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    " \n",
    "  # Lowercasing words\n",
    "  text = text.lower()\n",
    "  # Pass to lemmatizer\n",
    "  doc = nlp(text)\n",
    "  tokens = []\n",
    "  token_loss = []\n",
    "  token_loss_score = 0\n",
    "\n",
    "  # process each token\n",
    "  for token in doc:\n",
    "      if token.dep_ == 'neg':\n",
    "          head = token.head.lemma_\n",
    "          neg_token = 'not_' + head\n",
    "          tokens.append(neg_token)\n",
    "      elif token.lemma_.lower() not in stop_words and token.is_alpha:\n",
    "          tokens.append(token.lemma_)\n",
    "      elif token.is_alpha:\n",
    "        token_loss.append(token.lemma_)\n",
    "        token_loss_score += 1\n",
    "  return tokens, token_loss, token_loss_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Id2wU-N1Ua4K"
   },
   "outputs": [],
   "source": [
    "#Do cleaning only enough to get rid of fragments\n",
    "df['trope_description_partial_clean'] = df.apply(lambda x: remove_unneeded_phrases(x['trope_link'], x['trope_description']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_UDTYXeGgl7m"
   },
   "outputs": [],
   "source": [
    "#Do more traditional cleaning with getting rid of stopwords\n",
    "df[['trope_description_trad_clean','d_token_loss','d_loss_scores']] = df['trope_description_partial_clean'].apply(preprocess_text).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QSTGNzd6iooB"
   },
   "outputs": [],
   "source": [
    "#Flatten from the traditional cleaning\n",
    "df['trope_description_trad_clean_flattened'] = df['trope_description_trad_clean'].apply(lambda x: ' '.join(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trope_name</th>\n",
       "      <th>trope_link</th>\n",
       "      <th>trope_description</th>\n",
       "      <th>related_link_names</th>\n",
       "      <th>related_links</th>\n",
       "      <th>example_descriptions</th>\n",
       "      <th>example_related_links</th>\n",
       "      <th>example_related_link_names</th>\n",
       "      <th>trope_description_partial_clean</th>\n",
       "      <th>trope_description_trad_clean</th>\n",
       "      <th>d_token_loss</th>\n",
       "      <th>d_loss_scores</th>\n",
       "      <th>trope_description_trad_clean_flattened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aardvark Trunks</td>\n",
       "      <td>http://tvtropes.org/pmwiki/pmwiki.php/Main/Aar...</td>\n",
       "      <td>['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Main/SmallTaxonomyPools',...</td>\n",
       "      <td>['are more rarely seen', 'Real Life', 'sucking...</td>\n",
       "      <td>[\"Princess Tutu : Anteaterina's snout is rathe...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Anime/PrincessTutu', '/pm...</td>\n",
       "      <td>['Princess Tutu', \"Boner's Ark\", 'Cerebus the ...</td>\n",
       "      <td>['', '', ' ', '', \"Anteaters and aardvarks are...</td>\n",
       "      <td>[anteater, aardvark, somewhat, common, fiction...</td>\n",
       "      <td>[and, be, in, while, other, such, as, and, be,...</td>\n",
       "      <td>77</td>\n",
       "      <td>anteater aardvark somewhat common fiction ant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abandoned Area</td>\n",
       "      <td>http://tvtropes.org/pmwiki/pmwiki.php/Main/Aba...</td>\n",
       "      <td>['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Main/SceneryGorn', '/pmwi...</td>\n",
       "      <td>['Scenery Gorn', 'After the End', 'explain', '...</td>\n",
       "      <td>['MD Geist has plenty of these areas on Planet...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Anime/MDGeist', '/pmwiki/...</td>\n",
       "      <td>['MD Geist', 'Spirited Away', 'Alabaster: The ...</td>\n",
       "      <td>['', '', ' Image from Pripyat, a ghost town ne...</td>\n",
       "      <td>[image, pripyat, ghost, town, near, chernobyl,...</td>\n",
       "      <td>[from, a, the, for, because, the, an, in, the,...</td>\n",
       "      <td>165</td>\n",
       "      <td>image pripyat ghost town near chernobyl nuclea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abandoned Camp Ruins</td>\n",
       "      <td>http://tvtropes.org/pmwiki/pmwiki.php/Main/Aba...</td>\n",
       "      <td>['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Main/DontGoInTheWoods', '...</td>\n",
       "      <td>['the woods', 'wolf', 'Grizzly bear', 'some ma...</td>\n",
       "      <td>['Quizzical : From Chapter 4, \"Heavy Traffic I...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Fanfic/Quizzical', '/pmwi...</td>\n",
       "      <td>['Quizzical', 'Annihilation (2018)', 'Anaconda...</td>\n",
       "      <td>['', '', ' ', '', \"You're lost in the woods , ...</td>\n",
       "      <td>[lose, wood, middle, night, creepy, sound, bra...</td>\n",
       "      <td>[you, in, the, it, the, of, the, and, all, the...</td>\n",
       "      <td>160</td>\n",
       "      <td>lose wood middle night creepy sound branch cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abandoned Catchphrase</td>\n",
       "      <td>http://tvtropes.org/pmwiki/pmwiki.php/Main/Aba...</td>\n",
       "      <td>['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Main/CharacterCatchphrase...</td>\n",
       "      <td>['Character Catchphrases', 'unwanted connotati...</td>\n",
       "      <td>['Birdie the Early Bird of McDonaldland had a ...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Advertising/McDonaldland'...</td>\n",
       "      <td>['McDonaldland', 'Bakemonogatari', 'Fist of th...</td>\n",
       "      <td>['', '', ' ', \"Character Catchphrases are a gr...</td>\n",
       "      <td>[character, catchphrase, great, simple, way, h...</td>\n",
       "      <td>[be, a, and, to, a, but, do, the, be, with, on...</td>\n",
       "      <td>56</td>\n",
       "      <td>character catchphrase great simple way help ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abandoned Hospital</td>\n",
       "      <td>http://tvtropes.org/pmwiki/pmwiki.php/Main/Aba...</td>\n",
       "      <td>['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Main/BedlamHouse', '/pmwi...</td>\n",
       "      <td>['mental institution', 'grisly', 'medical', 'e...</td>\n",
       "      <td>['The 6th episode of Bleach was set in an aban...</td>\n",
       "      <td>['/pmwiki/pmwiki.php/Manga/Bleach', '/pmwiki/p...</td>\n",
       "      <td>['Bleach', 'Don Kanonji', 'Death Note', 'Doubt...</td>\n",
       "      <td>['', '', ' ', '', '', 'The creepy abandoned ho...</td>\n",
       "      <td>[creepy, abandon, mental, institution, common,...</td>\n",
       "      <td>[the, be, a, in, in, and, be, all, other, and,...</td>\n",
       "      <td>224</td>\n",
       "      <td>creepy abandon mental institution common setti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              trope_name                                         trope_link  \\\n",
       "0        Aardvark Trunks  http://tvtropes.org/pmwiki/pmwiki.php/Main/Aar...   \n",
       "1         Abandoned Area  http://tvtropes.org/pmwiki/pmwiki.php/Main/Aba...   \n",
       "2   Abandoned Camp Ruins  http://tvtropes.org/pmwiki/pmwiki.php/Main/Aba...   \n",
       "3  Abandoned Catchphrase  http://tvtropes.org/pmwiki/pmwiki.php/Main/Aba...   \n",
       "4     Abandoned Hospital  http://tvtropes.org/pmwiki/pmwiki.php/Main/Aba...   \n",
       "\n",
       "                                   trope_description  \\\n",
       "0  ['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...   \n",
       "1  ['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...   \n",
       "2  ['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...   \n",
       "3  ['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...   \n",
       "4  ['http://tvtropes.org/pmwiki/pmwiki.php/Main/A...   \n",
       "\n",
       "                                  related_link_names  \\\n",
       "0  ['/pmwiki/pmwiki.php/Main/SmallTaxonomyPools',...   \n",
       "1  ['/pmwiki/pmwiki.php/Main/SceneryGorn', '/pmwi...   \n",
       "2  ['/pmwiki/pmwiki.php/Main/DontGoInTheWoods', '...   \n",
       "3  ['/pmwiki/pmwiki.php/Main/CharacterCatchphrase...   \n",
       "4  ['/pmwiki/pmwiki.php/Main/BedlamHouse', '/pmwi...   \n",
       "\n",
       "                                       related_links  \\\n",
       "0  ['are more rarely seen', 'Real Life', 'sucking...   \n",
       "1  ['Scenery Gorn', 'After the End', 'explain', '...   \n",
       "2  ['the woods', 'wolf', 'Grizzly bear', 'some ma...   \n",
       "3  ['Character Catchphrases', 'unwanted connotati...   \n",
       "4  ['mental institution', 'grisly', 'medical', 'e...   \n",
       "\n",
       "                                example_descriptions  \\\n",
       "0  [\"Princess Tutu : Anteaterina's snout is rathe...   \n",
       "1  ['MD Geist has plenty of these areas on Planet...   \n",
       "2  ['Quizzical : From Chapter 4, \"Heavy Traffic I...   \n",
       "3  ['Birdie the Early Bird of McDonaldland had a ...   \n",
       "4  ['The 6th episode of Bleach was set in an aban...   \n",
       "\n",
       "                               example_related_links  \\\n",
       "0  ['/pmwiki/pmwiki.php/Anime/PrincessTutu', '/pm...   \n",
       "1  ['/pmwiki/pmwiki.php/Anime/MDGeist', '/pmwiki/...   \n",
       "2  ['/pmwiki/pmwiki.php/Fanfic/Quizzical', '/pmwi...   \n",
       "3  ['/pmwiki/pmwiki.php/Advertising/McDonaldland'...   \n",
       "4  ['/pmwiki/pmwiki.php/Manga/Bleach', '/pmwiki/p...   \n",
       "\n",
       "                          example_related_link_names  \\\n",
       "0  ['Princess Tutu', \"Boner's Ark\", 'Cerebus the ...   \n",
       "1  ['MD Geist', 'Spirited Away', 'Alabaster: The ...   \n",
       "2  ['Quizzical', 'Annihilation (2018)', 'Anaconda...   \n",
       "3  ['McDonaldland', 'Bakemonogatari', 'Fist of th...   \n",
       "4  ['Bleach', 'Don Kanonji', 'Death Note', 'Doubt...   \n",
       "\n",
       "                     trope_description_partial_clean  \\\n",
       "0  ['', '', ' ', '', \"Anteaters and aardvarks are...   \n",
       "1  ['', '', ' Image from Pripyat, a ghost town ne...   \n",
       "2  ['', '', ' ', '', \"You're lost in the woods , ...   \n",
       "3  ['', '', ' ', \"Character Catchphrases are a gr...   \n",
       "4  ['', '', ' ', '', '', 'The creepy abandoned ho...   \n",
       "\n",
       "                        trope_description_trad_clean  \\\n",
       "0  [anteater, aardvark, somewhat, common, fiction...   \n",
       "1  [image, pripyat, ghost, town, near, chernobyl,...   \n",
       "2  [lose, wood, middle, night, creepy, sound, bra...   \n",
       "3  [character, catchphrase, great, simple, way, h...   \n",
       "4  [creepy, abandon, mental, institution, common,...   \n",
       "\n",
       "                                        d_token_loss  d_loss_scores  \\\n",
       "0  [and, be, in, while, other, such, as, and, be,...             77   \n",
       "1  [from, a, the, for, because, the, an, in, the,...            165   \n",
       "2  [you, in, the, it, the, of, the, and, all, the...            160   \n",
       "3  [be, a, and, to, a, but, do, the, be, with, on...             56   \n",
       "4  [the, be, a, in, in, and, be, all, other, and,...            224   \n",
       "\n",
       "              trope_description_trad_clean_flattened  \n",
       "0  anteater aardvark somewhat common fiction ant ...  \n",
       "1  image pripyat ghost town near chernobyl nuclea...  \n",
       "2  lose wood middle night creepy sound branch cre...  \n",
       "3  character catchphrase great simple way help ch...  \n",
       "4  creepy abandon mental institution common setti...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouptut \n",
    "df.to_csv('../data_clean/parsed_tropes_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs3HGS6OmNIU"
   },
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8U3E2Mqm0_vG"
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "def get_embedding(text):\n",
    "  embedding = model.encode(text)\n",
    "  return embedding\n",
    "def get_embedding_normalized(text):\n",
    "  embedding = model.encode(text)\n",
    "  normalized_embedding = normalize([embedding])[0]\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_embedding'] = df['trope_name'].apply(lambda x: get_embedding(x).tolist())\n",
    "df['n_embedding_normalized'] = df['trope_name'].apply(lambda x: get_embedding_normalized(x).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_columns(df:pd.DataFrame, column_name: str, norm: bool):\n",
    "    df_embedding = pd.DataFrame()\n",
    "    df_embedding['trope_name'] = df['trope_name']\n",
    "    \n",
    "    if norm:\n",
    "        print('In If statement, norm true')\n",
    "        df_embedding['n_embedding'] = df['n_embedding_normalized']\n",
    "        df_embedding['d_embedding'] = df[column_name].apply(lambda x: get_embedding_normalized(x).tolist())\n",
    "    else:\n",
    "        print('In If statement, norm false')\n",
    "        df_embedding['n_embedding'] = df['n_embedding']\n",
    "        df_embedding['d_embedding'] = df[column_name].apply(lambda x: get_embedding(x).tolist())\n",
    "        \n",
    "    return df_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame()\n",
    "df_test['trope_name'] = df['trope_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zACKgjDHn9vg"
   },
   "outputs": [],
   "source": [
    "embeddings_wanted = {'partial_clean_embeddings':'trope_description_partial_clean', 'partial_clean_embeddings_normalized': 'trope_description_partial_clean',\n",
    "                     'trad_clean_embeddings': 'trope_description_trad_clean', 'trad_clean_embeddings_normalized': 'trope_description_trad_clean'}\n",
    "output_path = '../data_clean/embeddings/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Normalized\n",
      "Past trope_name\n",
      "Past name embedding\n",
      "Printed partial_clean_embeddings at ../data_clean/embeddings/partial_clean_embeddings.\n",
      "Normalized\n",
      "Past trope_name\n",
      "Past name embedding\n",
      "Printed partial_clean_embeddings_normalized at ../data_clean/embeddings/partial_clean_embeddings_normalized.\n",
      "Not Normalized\n",
      "Past trope_name\n",
      "Past name embedding\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. the normalize function expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m df_embedding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPast name embedding\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m df_embedding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_embedding_normalized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m df_embedding\u001b[38;5;241m.\u001b[39mto_csv(full_path)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrinted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mpandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[26], line 21\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     19\u001b[0m df_embedding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPast name embedding\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m df_embedding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[value]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_embedding_normalized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     23\u001b[0m df_embedding\u001b[38;5;241m.\u001b[39mto_csv(full_path)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrinted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m, in \u001b[0;36mget_embedding_normalized\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding_normalized\u001b[39m(text):\n\u001b[0;32m      6\u001b[0m   embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[1;32m----> 7\u001b[0m   normalized_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1925\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1921\u001b[0m     sparse_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1923\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m-> 1925\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1927\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthe normalize function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_array_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1933\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1043\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1041\u001b[0m     )\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1046\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m   1049\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1050\u001b[0m         array,\n\u001b[0;32m   1051\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1052\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1053\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1054\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. the normalize function expected <= 2."
     ]
    }
   ],
   "source": [
    "for key, value in embeddings_wanted.items():\n",
    "    full_path = output_path + key\n",
    "    if 'normalized' in key:\n",
    "        print('Normalized')\n",
    "        df_embedding = pd.DataFrame()\n",
    "        df_embedding['trope_name'] = df['trope_name']\n",
    "        print('Past trope_name')\n",
    "        df_embedding['n_embedding'] = df['n_embedding_normalized']\n",
    "        print('Past name embedding')\n",
    "        df_embedding['d_embedding'] = df[value].apply(lambda x: get_embedding_normalized(x).tolist())\n",
    "        \n",
    "        df_embedding.to_csv(full_path)\n",
    "        print(f'Printed {key} at {full_path}.')\n",
    "    else:\n",
    "        print('Not Normalized')\n",
    "        df_embedding = pd.DataFrame()\n",
    "        df_embedding['trope_name'] = df['trope_name']\n",
    "        print('Past trope_name')\n",
    "        df_embedding['n_embedding'] = df['n_embedding']\n",
    "        print('Past name embedding')\n",
    "        df_embedding['d_embedding'] = df[value].apply(lambda x: get_embedding_normalized(x).tolist())\n",
    "        \n",
    "        df_embedding.to_csv(full_path)\n",
    "        print(f'Printed {key} at {full_path}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Normalized\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot Normalized\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     df_embedding \u001b[38;5;241m=\u001b[39m \u001b[43membedding_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     df_embedding\u001b[38;5;241m.\u001b[39mto_csv(full_path)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrinted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m, in \u001b[0;36membedding_columns\u001b[1;34m(df, column_name, norm)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     df_embedding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m     df_embeddding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_embedding\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m, in \u001b[0;36membedding_columns.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     df_embedding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m     df_embeddding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[column_name]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_embedding\n",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m, in \u001b[0;36mget_embedding\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(text):\n\u001b[1;32m----> 3\u001b[0m   embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:517\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    514\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 517\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    519\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:548\u001b[0m, in \u001b[0;36mMPNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    547\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids\u001b[38;5;241m=\u001b[39minput_ids, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds)\n\u001b[1;32m--> 548\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    557\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:338\u001b[0m, in \u001b[0;36mMPNetEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    336\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m--> 338\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    339\u001b[0m     hidden_states,\n\u001b[0;32m    340\u001b[0m     attention_mask,\n\u001b[0;32m    341\u001b[0m     head_mask[i],\n\u001b[0;32m    342\u001b[0m     position_bias,\n\u001b[0;32m    343\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    345\u001b[0m )\n\u001b[0;32m    346\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:308\u001b[0m, in \u001b[0;36mMPNetLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[0;32m    307\u001b[0m intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 308\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:275\u001b[0m, in \u001b[0;36mMPNetOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 275\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    277\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key, value in embeddings_wanted.items():\n",
    "    full_path = output_path + key\n",
    "    if 'normalized' in key:\n",
    "        print('Normalized')\n",
    "        df_embedding = embedding_columns(df, value, True)\n",
    "        df_embedding.to_csv(full_path)\n",
    "        print(f'Printed {key} at {full_path}.')\n",
    "    else:\n",
    "        print('Not Normalized')\n",
    "        df_embedding = embedding_columns(df, value, False)\n",
    "        df_embedding.to_csv(full_path)\n",
    "        print(f'Printed {key} at {full_path}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
